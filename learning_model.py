# Google Drive ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ import
import pandas as pd # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ìš©

# PyTorch ê´€ë ¨ import
import torch # ê¸°ë³¸ PyTorch ê¸°ëŠ¥
from torch.utils.data import Dataset, DataLoader # ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ë° ë°°ì¹˜ ë¡œë”©
from sklearn.preprocessing import LabelEncoder # ë¼ë²¨ â†’ ìˆ«ì ë³€í™˜
from sklearn.model_selection import train_test_split # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬

# Huggingface Transformers ê´€ë ¨ import
from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler  # í† í¬ë‚˜ì´ì €, ë¶„ë¥˜ ëª¨ë¸, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬

from torch.optim import AdamW  # ì˜µí‹°ë§ˆì´ì €
from transformers import get_scheduler # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬

# í•™ìŠµ ì§„í–‰ë¥  ì‹œê°í™” ê´€ë ¨ import
from tqdm import tqdm # ì§„í–‰ë¥  í‘œì‹œ

# ì‹œìŠ¤í…œ ê´€ë ¨ import
import os # íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë ¨ ì²˜ë¦¬

# ------------------------------ #
# ì„¤ì •
# ------------------------------ #
model_name = "madatnlp/km-bert" # ì˜í•™ íŠ¹í™” BERT
csv_path = "/content/drive/MyDrive/.ipynb_model_data/generated_disease_sentences_v2.csv"
batch_size = 16 # ë°°ì¹˜ í¬ê¸°
num_epochs = 3 # í•™ìŠµ íšŸìˆ˜
max_len = 512 # ì…ë ¥ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´
lr = 2e-5 # í•™ìŠµë¥  (learning rate)

# âœ… Google Drive ê²½ë¡œ ì„¤ì •
save_dir = "/content/drive/MyDrive/model_checkpoints" # ì €ì¥ í´ë”
os.makedirs(save_dir, exist_ok=True) # í´ë” ì—†ìœ¼ë©´ ìƒì„±

# ------------------------------ #
# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# ------------------------------ #
df = pd.read_csv(csv_path) # CSV íŒŒì¼ì—ì„œ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
num_records = len(df) # ë ˆì½”ë“œ ìˆ˜ ê³„ì‚° (í—¤ë” ì œì™¸)

label_encoder = LabelEncoder() # ì§ˆë³‘ëª… ë¼ë²¨ ì¸ì½”ë” ìƒì„±
df["label"] = label_encoder.fit_transform(df["disease_name"]) # ì§ˆë³‘ëª…ì„ ìˆ«ì ë¼ë²¨ë¡œ ë³€í™˜
num_classes = len(label_encoder.classes_) # ì „ì²´ í´ë˜ìŠ¤(ì§ˆë³‘) ê°œìˆ˜ ê³„ì‚°

# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (80% í•™ìŠµ, 20% ê²€ì¦)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["generated_sentence"].tolist(), df["label"].tolist(), test_size=0.2, random_state=42
)

# ------------------------------ #
# 2. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ ì •ì˜
# ------------------------------ #
# KM-BERT í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = BertTokenizer.from_pretrained(model_name)

# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹
class DiseaseDataset(Dataset):
    def __init__(
        self,
        texts, # ì…ë ¥ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        labels # ì •ë‹µ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
    ):
        # í† í°í™” ë° íŒ¨ë”©
        self.encodings = tokenizer(
            texts,
            truncation=True, # max_lengthë³´ë‹¤ ê¸¸ë©´ ìë¦„
            padding=True, # ì§§ì€ ë¬¸ì¥ì€ max_lengthì— ë§ê²Œ íŒ¨ë”©
            max_length=max_len # ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •
        )
        self.labels = labels # ì •ë‹µ ë¼ë²¨ ì €ì¥

    # ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì • ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜
    def __getitem__(
        self,
        idx # ìš”ì²­í•œ ë°ì´í„°ì˜ ì¸ë±ìŠ¤
    ):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()} # ì…ë ¥ê°’ì„ í…ì„œë¡œ ë³€í™˜
        item["labels"] = torch.tensor(self.labels[idx]) # ë¼ë²¨ ì¶”ê°€
        return item # í•˜ë‚˜ì˜ í•™ìŠµ ìƒ˜í”Œ ë°˜í™˜

    def __len__(self):
        return len(self.labels)

# ë°ì´í„°ì…‹ ìƒì„±
train_dataset = DiseaseDataset(train_texts, train_labels) # í•™ìŠµìš© ë°ì´í„°ì…‹
val_dataset = DiseaseDataset(val_texts, val_labels) # ê²€ì¦ìš© ë°ì´í„°ì…‹

# ë°ì´í„°ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # í•™ìŠµìš© ë°ì´í„°ë¡œë”
val_loader = DataLoader(val_dataset, batch_size=batch_size) # ê²€ì¦ìš© ë°ì´í„°ë¡œë”

# ------------------------------ #
# 3. ëª¨ë¸ êµ¬ì„± ë° ìµœì í™”ê¸°
# ------------------------------ #
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes) # KM-BERT ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™” (í´ë˜ìŠ¤ ìˆ˜ ì§€ì •)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # GPU ì‚¬ìš© ì—¬ë¶€ ì„¤ì •
model.to(device) # ëª¨ë¸ì„ GPU ë˜ëŠ” CPUì— ë¡œë“œ

optimizer = AdamW(model.parameters(), lr=lr) # Adamw ì˜µí‹°ë§ˆì´ì € ì„¤ì •
lr_scheduler = get_scheduler( # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    "linear", # ì„ í˜• ê°ì†Œ ìŠ¤ì¼€ì¤„
    optimizer=optimizer, # ì ìš©í•  ì˜µí‹°ë§ˆì´ì €
    num_warmup_steps=0, # ì›Œë°ì—… ë‹¨ê³„ ì—†ìŒ
    num_training_steps=len(train_loader) * num_epochs # ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ ì„¤ì •
)

# ------------------------------ #
# 4. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì´ì–´ í•™ìŠµ)
# ------------------------------ #
latest_epoch = 0 # ê¸°ë³¸ ì‹œì‘ epochë¥¼ 0ìœ¼ë¡œ ì„¤ì •
checkpoint_files = sorted([ # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì„ ì •ë ¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    f for f in os.listdir(save_dir) # ì €ì¥ ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ì„ ìˆœíšŒ
    if f.startswith(f"disease_classifier_epoch") and f.endswith(f"_{num_records}.pt") # ì§€ì •ëœ í˜•ì‹ì˜ íŒŒì¼ë§Œ í•„í„°ë§
])

if checkpoint_files: # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
    latest_checkpoint = os.path.join(save_dir, checkpoint_files[-1]) # ê°€ì¥ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    checkpoint = torch.load(latest_checkpoint, map_location=device, weights_only=False) # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    model.load_state_dict(checkpoint["model_state_dict"]) # ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ
    label_encoder = checkpoint["label_encoder"] # ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ
    latest_epoch = checkpoint["epoch"] # ë§ˆì§€ë§‰ í•™ìŠµëœ epoch ë¶ˆëŸ¬ì˜¤ê¸°
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {latest_checkpoint} | ì´ì–´ì„œ epoch {latest_epoch + 1}ë¶€í„° ì‹œì‘")
else: # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ì´ˆê¸° í•™ìŠµ ì‹œì‘ ì•ˆë‚´
    print("â„¹ï¸ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ. ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘")

# ------------------------------ #
# 5. í•™ìŠµ ë£¨í”„
# ------------------------------ #
for epoch in range( # í•™ìŠµì„ ì¬ê°œí•  epochë¶€í„° num_epochsê¹Œì§€ ë°˜ë³µ
    latest_epoch, 
    num_epochs
):
    model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    total_loss = 0  # ì—í­ ì „ì²´ ì†ì‹¤ ëˆ„ì 
    correct = 0  # ì •ë‹µ ë§ì¶˜ ê°œìˆ˜
    total = 0  # ì „ì²´ ìƒ˜í”Œ ê°œìˆ˜

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"): # í•™ìŠµ ë°ì´í„° ë°˜ë³µ
        batch = {k: v.to(device) for k, v in batch.items()} # ë°°ì¹˜ë¥¼ GPU ë˜ëŠ” CPUë¡œ ì´ë™
        outputs = model(**batch) # ëª¨ë¸ì— ë°°ì¹˜ ì…ë ¥
        loss = outputs.loss # ì†ì‹¤ ê³„ì‚°

        loss.backward() # ì—­ì „íŒŒ ìˆ˜í–‰
        optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        lr_scheduler.step() # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”

        total_loss += loss.item() # ì†ì‹¤ ëˆ„ì 

        preds = torch.argmax(outputs.logits, dim=1) # ê° ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì¶”ì¶œ
        labels = batch["labels"] # ì •ë‹µ ë ˆì´ë¸” ì¶”ì¶œ

        correct += (preds == labels).sum().item() # ë§ì¶˜ ìƒ˜í”Œ ìˆ˜ ëˆ„ì 
        total += labels.size(0) # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ëˆ„ì 

    accuracy = correct / total # ì •í™•ë„ ê³„ì‚°
    print(f"Epoch {epoch+1} Loss: {total_loss:.4f} | Accuracy: {accuracy:.4f}") # í˜„ì¬ ì—í­ì˜ ì†ì‹¤ê³¼ ì •í™•ë„ ì¶œë ¥

    # ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ìƒì„±
    checkpoint_path = os.path.join(save_dir, f"disease_classifier_epoch{epoch+1}_{num_records}.pt") 
    torch.save({ # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ 
        "epoch": epoch + 1, # í˜„ì¬ ì—í­ ë²ˆí˜¸
        "model_state_dict": model.state_dict(), # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
        "label_encoder": label_encoder, # ë¼ë²¨ ì¸ì½”ë” ì €ì¥
    }, checkpoint_path)
    
    # ì €ì¥ ì™„ë£Œ ë¡œê·¸ ì¶œë ¥
    print(f"ğŸ”„ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {checkpoint_path}") 

# ------------------------------ #
# 6. ìµœì¢… ëª¨ë¸ ì €ì¥
# ------------------------------ #
# ìµœì¢… ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
final_model_path = os.path.join(save_dir, f"disease_classifier_final_{num_records}.pt")

torch.save({ # ìµœì¢… ëª¨ë¸ ì €ì¥
    "model_state_dict": model.state_dict(), # ëª¨ë¸ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì €ì¥
    "label_encoder": label_encoder, # ë¼ë²¨ ì¸ì½”ë” ê°ì²´ ì €ì¥
}, final_model_path) 

# ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
