# Google Drive ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ import
import pandas as pd # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ìš©

# PyTorch ê´€ë ¨ import
import torch # ê¸°ë³¸ PyTorch ê¸°ëŠ¥
from torch.utils.data import Dataset, DataLoader # ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ë° ë°°ì¹˜ ë¡œë”©
from sklearn.preprocessing import LabelEncoder # ë¼ë²¨ â†’ ìˆ«ì ë³€í™˜
from sklearn.model_selection import train_test_split # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬

# Huggingface Transformers ê´€ë ¨ import
from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler  # í† í¬ë‚˜ì´ì €, ë¶„ë¥˜ ëª¨ë¸, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬

from torch.optim import AdamW  # ì˜µí‹°ë§ˆì´ì €
from transformers import get_scheduler # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬

# í•™ìŠµ ì§„í–‰ë¥  ì‹œê°í™” ê´€ë ¨ import
from tqdm import tqdm # ì§„í–‰ë¥  í‘œì‹œ

# ì‹œìŠ¤í…œ ê´€ë ¨ import
import os # íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë ¨ ì²˜ë¦¬

# ------------------------------ #
# ì„¤ì •
# ------------------------------ #
model_name = "madatnlp/km-bert" # ì˜í•™ íŠ¹í™” BERT
csv_path = "/content/drive/MyDrive/.ipynb_model_data/train_dataset.csv"
batch_size = 16 # ë°°ì¹˜ í¬ê¸°
num_epochs = 15 # í•™ìŠµ íšŸìˆ˜
max_len = 512 # ì…ë ¥ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´
lr = 2e-5 # í•™ìŠµë¥  (learning rate)

# Google Drive ê²½ë¡œ ì„¤ì •
save_dir = "/content/drive/MyDrive/model_checkpoints" # ì €ì¥ í´ë”
os.makedirs(save_dir, exist_ok=True) # í´ë” ì—†ìœ¼ë©´ ìƒì„±

# ------------------------------ #
# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# ------------------------------ #
df = pd.read_csv(csv_path, header=None)  # í—¤ë” ì—†ëŠ” CSV ì½ê¸°
df.columns = ["disease_name", "generated_sentence", "style_type"]  # ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì •

num_records = len(df)

label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["disease_name"])
num_classes = len(label_encoder.classes_)

# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (80% í•™ìŠµ, 20% ê²€ì¦)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["generated_sentence"].tolist(), df["label"].tolist(), test_size=0.2, random_state=42
)

# ------------------------------ #
# 2. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ ì •ì˜
# ------------------------------ #
# KM-BERT í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = BertTokenizer.from_pretrained(model_name)

# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹
class DiseaseDataset(Dataset):
    def __init__(
        self,
        texts, # ì…ë ¥ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        labels # ì •ë‹µ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
    ):
        # í† í°í™” ë° íŒ¨ë”©
        self.encodings = tokenizer(
            texts,
            truncation=True, # max_lengthë³´ë‹¤ ê¸¸ë©´ ìë¦„
            padding=True, # ì§§ì€ ë¬¸ì¥ì€ max_lengthì— ë§ê²Œ íŒ¨ë”©
            max_length=max_len # ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •
        )
        self.labels = labels # ì •ë‹µ ë¼ë²¨ ì €ì¥

    # ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì • ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜
    def __getitem__(
        self,
        idx # ìš”ì²­í•œ ë°ì´í„°ì˜ ì¸ë±ìŠ¤
    ):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()} # ì…ë ¥ê°’ì„ í…ì„œë¡œ ë³€í™˜
        item["labels"] = torch.tensor(self.labels[idx]) # ë¼ë²¨ ì¶”ê°€
        return item # í•˜ë‚˜ì˜ í•™ìŠµ ìƒ˜í”Œ ë°˜í™˜

    def __len__(self):
        return len(self.labels)

# ë°ì´í„°ì…‹ ìƒì„±
train_dataset = DiseaseDataset(train_texts, train_labels) # í•™ìŠµìš© ë°ì´í„°ì…‹
val_dataset = DiseaseDataset(val_texts, val_labels) # ê²€ì¦ìš© ë°ì´í„°ì…‹

# ë°ì´í„°ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # í•™ìŠµìš© ë°ì´í„°ë¡œë”
val_loader = DataLoader(val_dataset, batch_size=batch_size) # ê²€ì¦ìš© ë°ì´í„°ë¡œë”

# ------------------------------ #
# 3. ëª¨ë¸ êµ¬ì„± ë° ìµœì í™”ê¸°
# ------------------------------ #

# KM-BERT ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™” (í´ë˜ìŠ¤ ìˆ˜ ì§€ì •)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # GPU ì‚¬ìš© ì—¬ë¶€ ì„¤ì •
model.to(device) # ëª¨ë¸ì„ GPU ë˜ëŠ” CPUì— ë¡œë“œ

optimizer = AdamW(model.parameters(), lr=lr) # Adamw ì˜µí‹°ë§ˆì´ì € ì„¤ì •
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs)

# ------------------------------ #
# 4. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì´ì–´ í•™ìŠµ)
# ------------------------------ #
latest_epoch = 9
checkpoint_files = sorted([f for f in os.listdir(save_dir) if f.startswith(f"disease_classifier_epoch") and f.endswith(f"_{num_records}.pt")])

if checkpoint_files:
    latest_checkpoint = os.path.join(save_dir, checkpoint_files[-1])
    checkpoint = torch.load(latest_checkpoint, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint["model_state_dict"])
    label_encoder = checkpoint["label_encoder"]
    latest_epoch = checkpoint["epoch"]
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {latest_checkpoint} | ì´ì–´ì„œ epoch {latest_epoch + 1}ë¶€í„° ì‹œì‘")
else:
    print("â„¹ï¸ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ. ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘")

# ------------------------------ #
# 5. í•™ìŠµ ë£¨í”„
# ------------------------------ #
for epoch in range(latest_epoch, num_epochs):
    model.train()
    total_loss = 0  # ì—í­ ì „ì²´ ì†ì‹¤ ëˆ„ì 
    correct = 0  # ì •ë‹µ ë§ì¶˜ ê°œìˆ˜
    total = 0  # ì „ì²´ ìƒ˜í”Œ ê°œìˆ˜

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

        total_loss += loss.item()

        # ì˜ˆì¸¡ê°’ ê³„ì‚° (softmax ì—†ì´ argmaxë§Œìœ¼ë¡œ ê°€ëŠ¥)
        preds = torch.argmax(outputs.logits, dim=1)
        labels = batch["labels"]

        correct += (preds == labels).sum().item()  # ì •ë‹µ ê°œìˆ˜ ëˆ„ì 
        total += labels.size(0)  # ì „ì²´ ê°œìˆ˜ ëˆ„ì 

    accuracy = correct / total  # ì •í™•ë„ ê³„ì‚°
    print(f"Epoch {epoch+1} Loss: {total_loss:.4f} | Accuracy: {accuracy:.4f}")

    # ì¤‘ê°„ ì €ì¥
    checkpoint_path = os.path.join(save_dir, f"disease_classifier_epoch{epoch+1}_{num_records}.pt")
    torch.save({
        "epoch": epoch + 1,
        "model_state_dict": model.state_dict(),
        "label_encoder": label_encoder,
    }, checkpoint_path)
    print(f"ğŸ”„ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {checkpoint_path}")

# ------------------------------ #
# 6. ìµœì¢… ëª¨ë¸ ì €ì¥
# ------------------------------ #
final_model_path = os.path.join(save_dir, f"disease_classifier_final_{num_records}.pt")
torch.save({
    "model_state_dict": model.state_dict(),
    "label_encoder": label_encoder,
}, final_model_path)

print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
